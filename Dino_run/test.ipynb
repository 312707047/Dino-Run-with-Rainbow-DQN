{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gym_chrome_dino\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tensorflow.python.keras import callbacks\n",
    "from tqdm import tqdm\n",
    "from collections import deque, Counter\n",
    "from tensorflow.keras import layers, Input, Model\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "DISCOUNT = 0.99\n",
    "REPLAY_MEMORY_SIZE = 100000\n",
    "MIN_REPLAY_MEMORY_SIZE = 50000\n",
    "MINI_BATCH_SIZE = 32\n",
    "UPDATE_TARGET_EVERY = 5\n",
    "MODEL_NAME = 'Dino_run'\n",
    "MIN_REWARD = 200\n",
    "MEMORY_FRACTION = 0.2\n",
    "\n",
    "EPISODES = 20000\n",
    "\n",
    "epsilon = 0.1\n",
    "EPSILON_DECAY = 0.9975\n",
    "MIN_EPSILON = 0.0001\n",
    "\n",
    "AGGREGATE_STATS_EVERY = 100  # episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_rewards = [-200]\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# create model folder\n",
    "if not os.path.isdir('./models'):\n",
    "    os.makedirs('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        self._log_write_dir = self.log_dir\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "\n",
    "        self._train_dir = os.path.join(self._log_write_dir, 'train')\n",
    "        self._train_step = self.model._train_counter\n",
    "\n",
    "        self._val_dir = os.path.join(self._log_write_dir, 'validation')\n",
    "        self._val_step = self.model._test_counter\n",
    "\n",
    "        self._should_write_train_graph = False\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    def update_stats(self, **stats):\n",
    "        with self.writer.as_default():\n",
    "            for key, value in stats.items():\n",
    "                tf.summary.scalar(key, value, step = self.step)\n",
    "                self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        # main model\n",
    "        self.model = self.create_model()\n",
    "        # target model\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        # An array with last n steps for training\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "        \n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=\"logs/{}-{}\".format(MODEL_NAME, int(time.time())))\n",
    "        \n",
    "        self.target_update_counter = 0\n",
    "    \n",
    "    def create_model(self):\n",
    "        input = Input(shape=(150, 600, 1)) # env.current_frame.shape\n",
    "        x = layers.Cropping2D(cropping=((0, 0), (0, 150)))(input)\n",
    "        x = layers.Resizing(120, 120)(x)\n",
    "        x = layers.Conv2D(32, 8, activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        # x = layers.Dropout(0.2)(x)\n",
    "        x = layers.Conv2D(64, 4, activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        # x = layers.Dropout(0.2)(x)\n",
    "        x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        # x = layers.Dropout(0.2)(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        # x = layers.Dropout(0.2)(x)\n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        # x = layers.Dropout(0.2)(x)\n",
    "        output = layers.Dense(env.action_space.n, activation='linear')(x) # use linear because we use np.argmax\n",
    "        \n",
    "        model = Model(input, output)\n",
    "        \n",
    "        model.compile(loss='mse',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['mse'])\n",
    "        return model\n",
    "    \n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "    \n",
    "    # train network every step during episode\n",
    "    def train(self, terminal_state, step):\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "\n",
    "        # Get a minibatch of random samples from memory replay table\n",
    "        minibatch = random.sample(self.replay_memory, MINI_BATCH_SIZE)\n",
    "\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch])/255\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "        # print(current_qs_list) [[a, b, c, ...]] a, b, c shape=(9,)\n",
    "\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])/255\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # Update Q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            # print(current_qs)\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "        # Fit on all samples as one batch, log only on terminal state\n",
    "        self.model.fit(np.array(X)/255, np.array(y), batch_size=MINI_BATCH_SIZE, verbose=0, shuffle=False, callbacks=[self.tensorboard] if terminal_state else None)\n",
    "\n",
    "        # Update target network counter every episode\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    # Queries main network for Q values given current observation space (environment state)\n",
    "    def get_qs(self, state):\n",
    "        # return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\n",
    "        return self.model.predict(np.array(np.expand_dims(state, axis=0))/255)[0]\n",
    "\n",
    "env = gym.make('ChromeDino-v0')\n",
    "# env = SubprocVecEnv([env_lambda for _ in range(4)])\n",
    "agent = DQNAgent()\n",
    "\n",
    "\n",
    "for episode in tqdm(range(1, EPISODES+1), ascii=True, unit='episodes'):\n",
    "    \n",
    "    # agent.tensorboard.step = episode\n",
    "    \n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "    \n",
    "    current_state = env.reset() # (150, 600, 1)\n",
    "    # stack_state = np.stack((current_state, current_state, current_state, current_state), axis=2)\n",
    "    # not_random = 0\n",
    "    # randoms = 0\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(agent.get_qs(current_state))\n",
    "            # not_random +=1\n",
    "        else:\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "            # randoms +=1\n",
    "        \n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        agent.train(done, step)\n",
    "        \n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "    # print(f'total random walk: {randoms} | total not random: {not_random} | epsilon\" {epsilon}')\n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        # agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\n",
    "        \n",
    "        if min_reward >= MIN_REWARD:\n",
    "            agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "        \n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ChromeDino-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state = env.reset()\n",
    "# cv2.imshow('dino', current_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.uniform([150, 600, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150, 600, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(np.expand_dims(a, axis=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac3da779536756720bc930bbdcbe3b303a716c4190960bb8b007750e7b6b7c5d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
